{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1206ea23",
   "metadata": {},
   "source": [
    "## A network analysis of the texts in books of hours\n",
    "\n",
    "The research project aims to answer the following questions: \n",
    "    \n",
    "* Which text co-occur in books? In other words, which texts were often combined in books of hours? \n",
    "* Which texts were unique?\n",
    "\n",
    "\n",
    "\n",
    "Which texts co-occur in books?\n",
    "\n",
    "Collect all texts they share (through an analysis of the paths; betweenness centrality)\n",
    "\n",
    "Which texts are unique to specific books?\n",
    "\n",
    "Are there differences per century?\n",
    "\n",
    "\n",
    "\n",
    "Handschriften die samen voorkomen:\n",
    "kortste pad van 3\n",
    "Tel het aantal paden\n",
    "Maak gephi-bestand\n",
    "Voor totaal en per eeuw\n",
    "\n",
    "Visualisatie in heatmap\n",
    "\n",
    "Identificeer alle handschriften die maar een keer voorkomen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892898a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import bnm\n",
    "from tdmh import *\n",
    "from operator import itemgetter\n",
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "from tdmh import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcdb64f",
   "metadata": {},
   "source": [
    "## Creating the data set\n",
    "\n",
    "To address this question, we firstly select the data we work with. The data have all been exported from the BNM-i, a database contructed by the Huyghens ING. The texts have all been saved as separate JSON files. 41902 files have been dowbloaded in total. The code below selects the texts which have been assigned a category containing the words 'getijden' or 'gebeden'. This is the case for 5437 texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('BNM_texts')\n",
    "print(f'{len(files)} texts in total.')\n",
    "\n",
    "selected_texts = []\n",
    "\n",
    "for file in files:\n",
    "    if re.search( r'.json$' , file ):\n",
    "        path = os.path.join('BNM_texts' , file)\n",
    "        json_str = open( path , encoding = 'utf-8')\n",
    "        json_data = json.load(json_str)\n",
    "        \n",
    "        categories = bnm.get_categories(json_data)\n",
    "        for c in categories:\n",
    "            \n",
    "            if re.search( r'\\bgetijden' , c ) or re.search( r'\\bgebeden' , c ):\n",
    "                selected_texts.append( path )\n",
    "            \n",
    "selected_texts = list(set(selected_texts))\n",
    "print( f'{len(selected_texts)} texts selected.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb14fa",
   "metadata": {},
   "source": [
    "A further selection takes place. We focus exclusively on the texts that have been assigned a standardised title.\n",
    "\n",
    "The code below navigates across all the texts, and establishes the carriers (or the books) these texts are in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02530e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open('books_of_hours.tsv' , 'w' , encoding = 'utf-8')\n",
    "out.write('text_id\\ttext_title\\tbook_id\\tbook_title\\tyear\\n')\n",
    "\n",
    "count = 0 \n",
    "norm_titles = []\n",
    "\n",
    "\n",
    "for text in selected_texts:\n",
    "    json_str = open( text, encoding = 'utf-8')\n",
    "    json_data = json.load(json_str)\n",
    "    #print(text)\n",
    "    norm_title = bnm.get_norm_title(json_data)\n",
    "\n",
    "    if len(norm_title) > 0:\n",
    "        #print(norm_title)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "        title = norm_title[0][1]\n",
    "        title = re.sub('Ongespecificeerde Mnl. teksten op naam van\\s+' , '' , str(title))\n",
    "        title = re.sub('Mnl. vertaling(en)? van\\s+' , '' , str(title))\n",
    "        title = re.sub('MNoordnederlandse vertaling van\\s+' , '' , str(title))     \n",
    "        \n",
    "        norm_titles.append(title)\n",
    "        # Find information about the book this text is in. \n",
    "        # the book is added to the modes dictionary\n",
    "        book = bnm.get_text_carrier(json_data)\n",
    "        \n",
    "        ## get the date of the book\n",
    "        path = os.path.join( 'BNM_carriers' , book[0][0] ) + '.json'\n",
    "        json_book = open( path , encoding = 'utf-8')\n",
    "        json_book = json.load(json_book)\n",
    "        book_date = json_book['datering']\n",
    "        \n",
    "        \n",
    "        # normalise book_date\n",
    "        date_norm = 0\n",
    "        if re.search( r'/' , str(book_date)):\n",
    "            parts = re.split( r'/' , book_date )\n",
    "            date_norm = ( int(parts[0]) + int(parts[1]) )/2\n",
    "        elif re.search( r'[?]{2}' , str(book_date)):\n",
    "            date_norm = re.sub( r'[?]{2}' , '50' , book_date )\n",
    "        elif book_date is not None:\n",
    "            date_norm = book_date\n",
    "        \n",
    "        if re.search( r'\\d' , str(date_norm) ):\n",
    "            date_norm = int(date_norm)\n",
    "        else:\n",
    "            date_norm = None\n",
    "        \n",
    "        \n",
    "        out.write(f'{norm_title[0][0]}\\t{title}\\t{book[0][0]}\\t{book[0][1]}\\t{date_norm}\\n')\n",
    "        \n",
    "\n",
    "out.close()\n",
    "\n",
    "print( f'{count} texts have been assigned normalised titles.' )\n",
    "print( f'There are {len(selected_texts)-count} without a normalised title.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd16a6",
   "metadata": {},
   "source": [
    "\n",
    "To be able to perform network analysis, we create an edges file and a nodes file. The nodes in this multimodal network are the texts and the books these texts are in. Note that the edges are directed: they represent the notion that a text occurs in a book. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_file = open('nodes.tsv' , 'w' , encoding = 'utf-8')\n",
    "edges_file = open('edges.csv' , 'w' , encoding = 'utf-8')\n",
    "\n",
    "nodes_file.write('Id\\tLabel\\tType\\n')\n",
    "edges_file.write('Source,Target\\n')\n",
    "\n",
    "titles_dict = dict()\n",
    "types_dict = dict()\n",
    "\n",
    "\n",
    "nodes = []\n",
    "\n",
    "df = pd.read_csv( 'books_of_hours.tsv' , sep = '\\t' )\n",
    "\n",
    "for i,row in df.iterrows():\n",
    "    nodes.append(row['text_id'])\n",
    "    titles_dict[ row['text_id'] ] = row['text_title'].strip()\n",
    "    types_dict[ row['text_id'] ] = 'Text'\n",
    "    \n",
    "    nodes.append(row['book_id'])\n",
    "    titles_dict[ row['book_id'] ] = row['book_title'].strip()\n",
    "    types_dict[ row['book_id'] ] = 'Book'\n",
    "    edges_file.write( f\"{row['text_id']},{row['book_id']}\\n\" )\n",
    "\n",
    "nodes = list(set(nodes))\n",
    "\n",
    "for n in nodes:\n",
    "    nodes_file.write( f\"{n}\\t{titles_dict[n]}\\t{types_dict[n]}\\n\" )\n",
    "\n",
    "edges_file.close()\n",
    "nodes_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ccde2",
   "metadata": {},
   "source": [
    "The collections of nodes are represented as Pandas data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(f'nodes.tsv' , sep = '\\t' )\n",
    "edges_df = pd.read_csv(f'edges.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf9f774",
   "metadata": {},
   "source": [
    "## Network analysis\n",
    "\n",
    "Now that we have all the nodes and the edges, we are ready to perform the network analysis. We firstly create a network of all the nodes. Texts are shown in orange, and the books are shown in blue. The visualisation reveals that there are a number of texts that appear in many different books. This is the case for 'Teksten op naam van Bernard Clairvaux' and 'Teksten op naam van Augustinus'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521dbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "net = Network(notebook=True , height=\"750px\", width=\"100%\" , bgcolor=\"#dce5f2\" )\n",
    "\n",
    "net.force_atlas_2based(\n",
    "        gravity=-60,\n",
    "        central_gravity=0.01,\n",
    "        spring_length=100,\n",
    "        spring_strength=0.08,\n",
    "        damping=0.4,\n",
    "        overlap= 0 )\n",
    "               \n",
    "for i,row in nodes_df.iterrows():\n",
    "    node = row['Id']\n",
    "    label= row['Label']\n",
    "    if row['Type'] == 'Text':\n",
    "        c ='#EE7733'\n",
    "    else:\n",
    "        c = '#007788'  \n",
    "    net.add_node( node , title=label,  color= c , value = 15 )\n",
    "                \n",
    "\n",
    "for i,row in edges_df.iterrows():\n",
    "    net.add_edge( row['Source'] , row['Target'] )\n",
    "                              \n",
    "\n",
    "\n",
    "net.show( f'network1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44087e",
   "metadata": {},
   "source": [
    "We can analyse the networks in Python using the `networkx` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5ed4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community \n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i,row in nodes_df.iterrows():\n",
    "    G.add_node( row['Id'] , type = row['Type'])\n",
    "                \n",
    "for i,row in edges_df.iterrows():\n",
    "    G.add_edge( row['Source'] , row['Target'] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c167c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763eaa7f",
   "metadata": {},
   "source": [
    "We want to establish the texts that co-occur in a book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e8c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = G.nodes()\n",
    "\n",
    "\n",
    "\n",
    "cooccurring_texts = dict()\n",
    "unique = []\n",
    "\n",
    "\n",
    "for node1 in all_nodes:\n",
    "    count = 0 \n",
    "    if types_dict[node1] == 'Text': \n",
    "        for node2 in all_nodes:\n",
    "            if types_dict[node2] == 'Text' and node1 != node2:\n",
    "                nr_paths = nx.all_simple_paths( G,node1,node2 , 3 )\n",
    "                for path in nr_paths:\n",
    "                    count += 1\n",
    "                    cooccurring_texts[(path[0],path[2]) ] = cooccurring_texts.get( (path[0],path[2]) , 0 )+1\n",
    "        if count == 0:\n",
    "            unique.append(node1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02318c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The following titles occur only once in the network:\\n')\n",
    "\n",
    "for t in unique:\n",
    "    print(f'{titles_dict[t]} ({t})' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abce9af1",
   "metadata": {},
   "source": [
    "This network can be plotted. The visualisation displays all the texts that cooccur in one or more books. It looks as if there are a number of 'cliques' consisting of texts that appear together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bc07e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "net = Network(notebook=True , height=\"750px\", width=\"100%\" , bgcolor=\"#dce5f2\" )\n",
    "\n",
    "net.force_atlas_2based(\n",
    "        gravity=-60,\n",
    "        central_gravity=0.01,\n",
    "        spring_length=100,\n",
    "        spring_strength=0.08,\n",
    "        damping=0.4,\n",
    "        overlap= 0 )\n",
    "               \n",
    "for node in cooccurring_texts:\n",
    "    net.add_node( node[0]  )\n",
    "    net.add_node( node[1]  )\n",
    "                \n",
    "\n",
    "for node in cooccurring_texts:\n",
    "    net.add_edge( node[0] , node[1] , value = cooccurring_texts[node] )\n",
    "                              \n",
    "\n",
    "net.show( f'network2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cd55bf",
   "metadata": {},
   "source": [
    "The information about the intensity of the cooccurrences (i.e. how often often do two diffent texts cooccur?) can be visualised by varying the thickness of the edges. Such a visualisation can also created in Gephi. The network should be imported as a non-directed graph. \n",
    "\n",
    "The cell below generates the CSV files that can be used for this purpose. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951710fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = open('cooccurrences_nodes.csv' , 'w')\n",
    "e = open('cooccurrences_edges.csv' , 'w')\n",
    "\n",
    "n.write('Id')\n",
    "e.write('Source,Target,Weight')\n",
    "\n",
    "all_nodes = []\n",
    "for node in cooccurring_texts:\n",
    "    all_nodes.append(node[0])\n",
    "    all_nodes.append(node[1])\n",
    "    \n",
    "all_nodes = list(set(all_nodes))\n",
    "\n",
    "for node in all_nodes:\n",
    "    n.write(f'{n}\\n')\n",
    "    \n",
    "for c in cooccurring_texts:\n",
    "    e.write(f'{c[0]},{c[1]},{cooccurring_texts[c]}\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmh import sortedByValue\n",
    "\n",
    "cooccurring_text_deduplicated = dict()\n",
    "for c in cooccurring_texts:\n",
    "    if (c[1],c[0]) not in cooccurring_text_deduplicated:\n",
    "        cooccurring_text_deduplicated[c] = cooccurring_texts[c]\n",
    "\n",
    "\n",
    "for c in sortedByValue(cooccurring_text_deduplicated , ascending = False ):\n",
    "    if cooccurring_text_deduplicated[c] > 1:\n",
    "        print( f'{titles_dict[c[0]]}({c[0]}) and {titles_dict[c[1]]}({c[1]}) occur together {cooccurring_text_deduplicated[c]} times \\n' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c2839",
   "metadata": {},
   "source": [
    "The cell below identifies the texts that occur most frequently with other texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d508e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree(G.nodes()))\n",
    "\n",
    "for d in sortedByValue( degrees , ascending = False ):\n",
    "    if degrees[d] > 1:\n",
    "        print( f'{titles_dict[d]} ({d}) => {degrees[d] }' )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960c8e7",
   "metadata": {},
   "source": [
    "The following texts are unique in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3275b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sortedByValue( degrees , ascending = False ):\n",
    "    if degrees[d] == 1 and types_dict[d] == 'Text':\n",
    "        print( f'{titles_dict[d]} ({d})' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2e7d8b",
   "metadata": {},
   "source": [
    "Which books do these texts appear in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15954f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sortedByValue(cooccurring_text_deduplicated , ascending = False ):\n",
    "    if cooccurring_text_deduplicated[c] > 1:\n",
    "        nr_paths = nx.all_simple_paths( G,c[0],c[1] , 3 )\n",
    "        for path in nr_paths:\n",
    "            print(f'{path[0]} and {path[2]} both occur in ')\n",
    "            print( f'{titles_dict[path[1]]} ({path[1]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"Network density: {nx.density(G) }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd30dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = G.nodes()\n",
    "\n",
    "\n",
    "all_books = []\n",
    "all_texts = []\n",
    "for node in all_nodes:\n",
    "    if types_dict[node] == 'Book': \n",
    "        all_books.append(node)\n",
    "    else:\n",
    "        all_texts.append(node)\n",
    "        \n",
    "books_dict = dict()\n",
    "\n",
    "\n",
    "## Create a list of all the texts in each book\n",
    "for book in all_books:\n",
    "    texts_list = []\n",
    "    for t in all_texts:\n",
    "        nr_paths = nx.all_simple_paths( G,book,t , 2 )\n",
    "        for path in nr_paths:\n",
    "            texts_list.append( path[1] )\n",
    "    books_dict[book] = texts_list\n",
    "    \n",
    "## next, create an overview of the number of texts the books have in common\n",
    "\n",
    "books_edges = dict()\n",
    "\n",
    "for book1 in books_dict:\n",
    "    for book2 in books_dict:\n",
    "        if book1 != book2:\n",
    "            \n",
    "            intersection = list(set(books_dict[book1]) & set(books_dict[book2]))\n",
    "            if len(intersection) > 2:\n",
    "                books_edges[(book1,book2)] = len(intersection)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f6f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbn = open( 'similar_books_nodes.csv' , 'w' ) \n",
    "sbe = open( 'similar_books_edges.csv' , 'w' )\n",
    "\n",
    "sbn.write('Id\\n')\n",
    "sbe.write('Source,Target\\n')\n",
    "\n",
    "\n",
    "nodes = []\n",
    "\n",
    "for be in books_edges:\n",
    "    sbe.write(f'{be[0]},{be[1]}\\n')\n",
    "    if be[0] not in nodes:\n",
    "        nodes.append(be[0])\n",
    "    if be[1] not in nodes:\n",
    "        nodes.append(be[0])\n",
    "\n",
    "for n in nodes:\n",
    "    sbn.write(f'{n}\\n')\n",
    "    \n",
    "sbn.close()\n",
    "sbe.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3305316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = Network(notebook=True , height=\"750px\", width=\"100%\" , bgcolor=\"#dce5f2\" )\n",
    "\n",
    "net.force_atlas_2based(\n",
    "        gravity=-60,\n",
    "        central_gravity=0.01,\n",
    "        spring_length=100,\n",
    "        spring_strength=0.08,\n",
    "        damping=0.4,\n",
    "        overlap= 0 )\n",
    "               \n",
    "for node in books_edges:\n",
    "    net.add_node( node[0]  )\n",
    "    net.add_node( node[1]  )\n",
    "                \n",
    "\n",
    "for node in books_edges:\n",
    "    net.add_edge( node[0] , node[1] , value = books_edges[node] )\n",
    "                              \n",
    "\n",
    "net.show( f'network3.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c8a29",
   "metadata": {},
   "source": [
    "## Some other analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43025b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "betweenness_dict = nx.betweenness_centrality(G) \n",
    "eigenvector_dict = nx.eigenvector_centrality(G) \n",
    "\n",
    "nx.set_node_attributes(G, betweenness_dict, 'betweenness')\n",
    "nx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "\n",
    "sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"5 nodes with the higest betweenness centrality:\")\n",
    "for b in sorted_betweenness[:5]:\n",
    "    print( f'{titles_dict[b[0]]} ({b[0]}), {b[1]} ' ) \n",
    "    \n",
    "print('\\n')    \n",
    "    \n",
    "sorted_eigen = sorted(eigenvector_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"5 nodes with the higest eigenvector centrality:\")\n",
    "for e in sorted_eigen[:5]:\n",
    "    print( f'{titles_dict[e[0]]} ({e[0]}), {e[1]} ' )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021da77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
